{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, activation_fun, leaky_alpha=0.01):\n",
    "        \"\"\"\n",
    "        Initializes a neural network object.\n",
    "        Parameters:\n",
    "        - activation_fun (str): The activation function to be used in the network.\n",
    "                               Options: \"relu\", \"leaky_relu\", or \"sigmoid\".\n",
    "        - leaky_alpha (float): The slope for the leaky ReLU activation function (default is 0.01).\n",
    "        \"\"\"\n",
    "        self.num_layers = 1 # input layer \n",
    "        self.neurons_per_layer = []\n",
    "        self.biases = []\n",
    "        self.weights = []\n",
    "        self.activation_fun_name = activation_fun  # store activation function name\n",
    "\n",
    "        # Set the activation function and its derivative\n",
    "        if self.activation_fun_name == \"relu\":\n",
    "            self.activation_fun = lambda x: np.maximum(x, 0)\n",
    "            self.activation_fun_d = lambda x: np.where(x > 0, 1, 0)\n",
    "        elif self.activation_fun_name == \"leaky_relu\":\n",
    "            self.leaky_alpha = leaky_alpha\n",
    "            self.activation_fun = lambda x: np.where(x > 0, x, self.leaky_alpha * x)\n",
    "            self.activation_fun_d = lambda x: np.where(x > 0, 1, self.leaky_alpha)\n",
    "        elif self.activation_fun_name == \"softplus\":\n",
    "            self.activation_fun = lambda x: np.log(1 + np.exp(x))\n",
    "            self.activation_fun_d = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_fun_name == \"sigmoid\":\n",
    "            self.activation_fun = lambda x: 1 / (1 + np.exp(-x))\n",
    "            self.activation_fun_d = lambda x: self.activation_fun(x) * (1 - self.activation_fun(x))\n",
    "            \n",
    "    \n",
    "    def layer(self, num_inputs, num_neurons):\n",
    "        \"\"\"Adds a new layer to the neural network.\n",
    "        Parameters:\n",
    "        num_inputs (int): Number of inputs or number of neurons of previous layer.\n",
    "        num_neurons (int): Number of neurons in the layer.\"\"\"\n",
    "        if self.num_layers == 1:\n",
    "            self.neurons_per_layer.append(num_inputs)\n",
    "        self.num_layers += 1\n",
    "        self.neurons_per_layer.append(num_neurons)\n",
    "        self.biases.append(np.random.randn(num_neurons, 1)*0.1) # gaussian distribution with mean 0 and standard deviation 0.1\n",
    "        self.weights.append(np.random.randn(num_neurons, num_inputs)*0.1)\n",
    "        \n",
    "    def forwardpass(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the neural network.\n",
    "        Parameters:\n",
    "        - x (ndarray): Input data.\n",
    "        Returns:\n",
    "        - ndarray: Output of the neural network.\n",
    "        \"\"\"\n",
    "        if self.activation_fun_name in [\"relu\", \"leaky_relu\", \"softplus\", \"sigmoid\"]:\n",
    "            for i in range(len(self.biases)-1):\n",
    "                x = self.activation_fun(np.dot(self.weights[i], x) + self.biases[i])\n",
    "            x = np.dot(self.weights[-1], x) + self.biases[-1]\n",
    "        #code for output layer with activation function\n",
    "        else:\n",
    "            for i in range(len(self.biases)):\n",
    "                x = self.activation_fun(np.dot(self.weights[i], x) + self.biases[i])\n",
    "        return x\n",
    "    \n",
    "    # loss function - MSE\n",
    "    def loss(self, x, y):\n",
    "        return (self.forwardpass(x) - y)**2\n",
    "    \n",
    "    def mean_loss(self, X, Y):\n",
    "        return np.mean([self.loss(x, y) for x, y in zip(X, Y)])\n",
    "    \n",
    "    def loss_d(self, activation, y):\n",
    "        return (activation - y) \n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Performs backpropagation through the neural network.\n",
    "        Parameters:\n",
    "        - x (ndarray): Input data.\n",
    "        - y (ndarray): Target data.\n",
    "        \"\"\"\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases] # initialize list for partial derivatives of the cost function with respect to the biases\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights] # initialize list for partial derivatives of the cost function with respect to the weights\n",
    "        nets = []\n",
    "        activations = [x] #first layer - x is input layer \n",
    "        if self.activation_fun_name in [\"relu\", \"leaky_relu\", \"softplus\", \"sigmoid\"]:\n",
    "            # forward pass\n",
    "            for i in range(len(self.biases)-1):\n",
    "                net = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
    "                nets.append(net)\n",
    "                activations.append(self.activation_fun(net))\n",
    "            # output layer without activation function\n",
    "            net = np.dot(self.weights[-1], activations[-1]) + self.biases[-1]\n",
    "            nets.append(net)\n",
    "            activations.append(net)\n",
    "            #backward pass - only for output layer\n",
    "            error = self.loss_d(activations[-1], y)\n",
    "            grad_b[-1] = error\n",
    "            grad_w[-1] = np.dot(error, activations[-2].T)\n",
    "        # code for output layer with activation function\n",
    "        else:\n",
    "            for i in range(len(self.biases)):\n",
    "                net = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
    "                nets.append(net)\n",
    "                activations.append(self.activation_fun(net))\n",
    "            error = self.loss_d(activations[-1], y) * self.activation_fun_d(nets[-1])\n",
    "            grad_b[-1] = error\n",
    "            grad_w[-1] = np.dot(error, activations[-2].T)\n",
    "        # backward pass for rest of the layers\n",
    "        for i in range(2, self.num_layers-1):\n",
    "            error = np.dot(self.weights[-i+1].T, error) * self.activation_fun_d(nets[-i])\n",
    "            grad_b[-i] = error\n",
    "            grad_w[-i] = np.dot(error, activations[-i-1].T)\n",
    "        return grad_b, grad_w\n",
    "\n",
    "    def fit(self, X, Y, epochs, learning_rate):\n",
    "        \"\"\"\n",
    "        Trains the neural network using full batch gradient descend.\n",
    "        Parameters:\n",
    "        - X (ndarray): Input data.\n",
    "        - y (ndarray): Target data.\n",
    "        - epochs (int): Number of epochs.\n",
    "        - learning_rate (float): Learning rate.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            grad_b_sum = [np.zeros(b.shape) for b in self.biases]\n",
    "            grad_w_sum = [np.zeros(w.shape) for w in self.weights]\n",
    "            for x,y in zip(X,Y):\n",
    "                grad_b, grad_w = self.backprop(x, y)\n",
    "                grad_b_sum = [gb + g for gb, g in zip(grad_b_sum, grad_b)]\n",
    "                grad_w_sum = [gw + g for gw, g in zip(grad_w_sum, grad_w)]\n",
    "            self.biases = [b - (learning_rate/len(x)) * gb for b, gb in zip(self.biases, grad_b_sum)]\n",
    "            self.weights = [w - (learning_rate/len(x)) * gw for w, gw in zip(self.weights, grad_w_sum)]\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(\"Epoch: {}, Loss: {}\".format(epoch+1, \n",
    "                                                   self.mean_loss(X, Y)))\n",
    "    def test(self, X, Y):\n",
    "        \"\"\"\n",
    "        Tests the neural network.\n",
    "        Parameters:\n",
    "        - X (ndarray): Input data.\n",
    "        - y (ndarray): Target data.\n",
    "        \"\"\"\n",
    "        for x,y in zip(X,Y):\n",
    "            print(\"Target: {}, Prediction: {}\".format(y, self.forwardpass(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('CrabAgePrediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "data_target = data['Age']\n",
    "data = data.drop(columns=['Age'])\n",
    "data['Sex'] = data['Sex'].map({'F': -1, 'I': 0, 'M': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data.loc[:, data.columns != 'Sex'])\n",
    "data_scaled = pd.DataFrame(data_scaled)\n",
    "data_scaled = pd.concat([data_scaled, data['Sex']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change data format for NN input\n",
    "x = [np.reshape(row, (8, 1)) for row in data_scaled.values]\n",
    "y = [np.reshape(row, (1, 1)) for row in data_target.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create NN models with different layers and neurons\n",
    "models = []\n",
    "i = 0 \n",
    "for layers in reversed(range(1,5)):\n",
    "    for neurons in reversed([4,8,16,32]):\n",
    "        for activation in [\"relu\"]:\n",
    "            for iter in range(4):\n",
    "                i += 1 \n",
    "                model = NN(activation)\n",
    "                model.layer(8, neurons)\n",
    "                for _ in range(layers-1):\n",
    "                    model.layer(neurons, neurons)\n",
    "                model.layer(neurons, 1)\n",
    "                model.fit(x_train, y_train, 500, 0.0001)\n",
    "                print(\"{}/64\\nLayers: {}, Neurons: {}, Activation: {}, Loss: {}, Iter = {}\".format(i,layers, neurons, activation, model.mean_loss(x_test, y_test), iter+1))\n",
    "                models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create NN models with different activation functions\n",
    "models_activations = []\n",
    "i = 0 \n",
    "for layers in [2]:\n",
    "    for neurons in [32]:\n",
    "        for activation in [\"relu\", \"leaky_relu\", \"softplus\", \"sigmoid\"]:\n",
    "            for iter in range(4):\n",
    "                i += 1 \n",
    "                model = NN(activation)\n",
    "                model.layer(8, neurons)\n",
    "                for _ in range(layers-1):\n",
    "                    model.layer(neurons, neurons)\n",
    "                model.layer(neurons, 1)\n",
    "                model.fit(x_train, y_train, 1000, 0.0001)\n",
    "                print(\"{}/48\\nLayers: {}, Neurons: {}, Activation: {}, Loss: {}, Iter = {}\".format(i,layers, neurons, activation, model.mean_loss(x_test, y_test), iter+1))\n",
    "                models_activations.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results to excel file\n",
    "df = pd.read_excel(\"Wyniki.xlsx\")\n",
    "for model in models:    \n",
    "    df_result = pd.DataFrame([model.mean_loss(x_test, y_test), model.num_layers, model.neurons_per_layer, model.activation_fun_name]).T\n",
    "    df_result.columns = ['Loss', 'Layers', 'Neurons', 'Activation']\n",
    "    df = pd.concat([df, df_result], ignore_index=True)\n",
    "df.to_excel(\"Wyniki.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results to excel file\n",
    "df = pd.read_excel(\"Activations.xlsx\")\n",
    "for model in models_activations:    \n",
    "    df_wynik = pd.DataFrame([model.mean_loss(x_test, y_test), model.num_layers, model.neurons_per_layer, model.activation_fun_name]).T\n",
    "    df_wynik.columns = ['Loss', 'Layers', 'Neurons', 'Activation']\n",
    "    df = pd.concat([df, df_result], ignore_index=True)\n",
    "df.to_excel(\"Activations.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
